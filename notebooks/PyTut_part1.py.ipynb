{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part 1 of a tutorial series that will get you up and running in pytorch. \n",
    "\n",
    "In this section we will cover 5 major things.\n",
    "- Tensors(Making them, Using them, Converting from numpy)\n",
    "- nn.Sequential for basic models\n",
    "- Defining our own model class\n",
    "- Running on GPU\n",
    "- Running on multiple GPU (very basic introduction)\n",
    "\n",
    "\n",
    "This tutorial will be a high level overview to get you started as quickly as possible, if you need or want more in depth descriptions for anything discussed here then check out the official [pytorch guides](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch generally\n",
    "import torch.nn as nn # module used for neural networks\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to start with defining a few Tensors. You can think of a tensor as a type of array specific to the PyTorch library. Almost all operations are done through tensors so it's best to get used to working with them over numpy arrays, however there does exist easy ways to convert between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,5) # Define a tensor of random numbers of shape (1,5)\n",
    "print(x.shape)\n",
    "y = torch.zeros(5,1) # Define a tensor of all zeros of shape (5,1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, every tensor has a data type with the default being float32. When initializing a tensor you can usually specifiy a data type by passing in a dtype kwarg. Alternatively, you can set it after the tensor is already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "x = x.double() # does not change in place!\n",
    "print(x.dtype)\n",
    "y = y.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 2 tensors we can do some simple operations on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's go over the two ways to convert a numpy array into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "\n",
    "# Convert by first converting back to python list\n",
    "# not advised for all situations\n",
    "a_as_tensor = torch.Tensor(a.tolist())\n",
    "\n",
    "# built in helper function for this exact task. Use this as your first choice\n",
    "a_as_tensor = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have just about the same available operations as a numpy array, so I'm avoiding going too in depth with the usability as I assume the reader has some experience working with numpy. For a full list of availalbe methods go [here](https://pytorch.org/docs/stable/torch.html#tensors). \n",
    "\n",
    "### Now, we're going to make our first model\n",
    "\n",
    "We will be working with random data as the goal here is not to make a true working neural network but rather to just get a feel for the syntax and design choices of the PyTorch framework. Since we work in NLP and most of our models end up being variants of recurrent neural networks(RNN), the examples here will use the Gated Recurrent Unit(GRU) cell. For those unfamiliar, a very general description of an RNN is a neural network that stacks n layers of architecture where each layer is given a new input vector as well as a secondary input vector from the previous layer(i.e. recurrent). This is greatly helpful for most sequence modeling tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5\n",
    "seq_len = 10\n",
    "input_size = 50\n",
    "# 5 examples, each with sequence length of 10, with each sequence element being 50-dimensional\n",
    "data = torch.randn(num_examples, seq_len, input_size) \n",
    "labels = torch.zeros(5) # all labels are zero because why not\n",
    "\n",
    "\n",
    "# Model Hyperparams\n",
    "hidden_size = 35 # amount of hidden neurons in our rnn layer\n",
    "dropout_rate = 0.0 # amount of dropout as defined in a probability (0-1)\n",
    "bidirectional = False # if we want a bidirectional RNN\n",
    "stacked_layers = 1 # number of RNNs we want to stack\n",
    "\n",
    "# we need to define this function to get GRU to work in the sequential layout\n",
    "# GRU returns a tuple of (output, hidden_state) but the final linear layer only expects 1 value\n",
    "# so we just drop the final hidden state in this forward definition\n",
    "class DropHidden(nn.Module):\n",
    "    def forward(self, x):\n",
    "        output, hidden = x[0], x[1]\n",
    "        return output\n",
    "\n",
    "# nn.sequential is a quick way to create a forward pass of your data in terms of neural network layers\n",
    "# It's not advised to use this for anything more than basic prototyping as you're restricted to only\n",
    "# a declarative approach to coding\n",
    "model = nn.Sequential(\n",
    "    nn.GRU(input_size, hidden_size, batch_first=True), # batch_first means first dim is size of batch\n",
    "    DropHidden(),\n",
    "    nn.Linear(hidden_size, 1) # fully connected layer to do a regression\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train your model\n",
    "\n",
    "PyTorch offers some helpful classes for organizing your train/test data in a way that removes a lot of boiler plate for looping/etc. These are called DataSets and DataLoaders, essentially a DataSet class organizes your data into pairs of (examples, labels) and the DataLoader can handle splitting it into batches or shuffling of the DataSet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "\n",
    "train_data = utils.TensorDataset(data, labels)\n",
    "train_dataloader = utils.DataLoader(train_data, batch_size=1, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally, we need to define 2 very important aspects of our network\n",
    "\n",
    "First, we need to define the optimizer we are going to use to update our network parameters. The most common of these is stochastic gradient descent, but from experience for RNNs I find [ADAM](https://arxiv.org/abs/1412.6980) to work well. Second, we need to pick a loss function. A loss function is essentially what tells your network how 'wrong' or 'right' it is when preforming a prediction and is the starting point for back-prop through the network for updating parameters. The problem laid out in this tutorial is a simple regression so we will stick with mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to write our first training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0504742007702589\n"
     ]
    }
   ],
   "source": [
    "def train_model(model):\n",
    "    # very important to put model into 'training mode'\n",
    "    # Turns ON dropout and a few other aspects\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, label in train_dataloader:\n",
    "        optimizer.zero_grad() # zero out optimizer for a new batch\n",
    "        output = model(inputs) # pass to our defined model\n",
    "        \n",
    "        batch_loss = criterion(output, label) # calculate loss\n",
    "        total_loss += batch_loss.item() # get numeric value to track loss (print per epoch/etc)\n",
    "        batch_loss.backward() # trigger backprop\n",
    "        optimizer.step() # update network based on backprop results\n",
    "        \n",
    "    print(total_loss / 5) # print 'average' loss across all batches\n",
    "    \n",
    "train_model(model) # run the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential clearly has its limits\n",
    "\n",
    "As we saw using sequential was great for prototyping but already started to show limitations(it doesn't even natively work for RNNs without adding custom code anyway!). The suggested approach for coding in PyTorch is to define your own class for your model. You start by extending the nn.Module from PyTorch and then defining the necessary layers of your network. Once you have your layers defined the key function to implement is forward, this function will do an entire forward pass of your network similar to sequential. If you need more fine-grained control over your model then you can implement a step function and write a loop in your forward function that calls step, if for some reason you need to write a custom network that requires a calculation at each forward movement in the graph that would be the best approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, drop=0, bidirect=False, batch_first=True, layers=1):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=layers, dropout=drop, batch_first=True, bidirectional=bidirect)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.bidirect = bidirect\n",
    "        self.num_layers = layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "            \n",
    "        output, hidden = self.gru(inputs, hidden)\n",
    "        return self.linear(output)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters())\n",
    "        if self.bidirect:\n",
    "            return weight.new_zeros(2*self.num_layers, 1, self.hidden_size)\n",
    "        else:\n",
    "            return weight.new_zeros(self.num_layers, 1, self.hidden_size)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026621105894446374\n"
     ]
    }
   ],
   "source": [
    "model = myGRU(input_size, hidden_size)\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the above code was running on the CPU, what about GPU?\n",
    "\n",
    "At some point you'll be creating a model that takes far too long to train using simply the CPU. If you're running for 50 epochs and each epoch is taking an hour that's just not viable. PyTorch offers a very simple way to move your code to the GPU for a massive speed boost. As long as you have an nvidia GPU and configured CUDA this should work easily(Hercules as a server is configured, so running there is fine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n",
      "0.03352753575891256\n"
     ]
    }
   ],
   "source": [
    "# first check to make sure pytorch recognizes your GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('yay')\n",
    "else:\n",
    "    print('boo')\n",
    "    \n",
    "    \n",
    "# First move your model, criterion to cuda\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "    # need to update optimizer with the params that are now ON the GPU\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
    "    \n",
    "def train_model_cuda(model):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, label in train_dataloader:\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            label = label.cuda()\n",
    "        \n",
    "        output = model(inputs) \n",
    "        \n",
    "        batch_loss = criterion(output.cuda(), label) \n",
    "        total_loss += batch_loss.item()\n",
    "        batch_loss.backward() \n",
    "        optimizer.step()\n",
    "    \n",
    "    print(total_loss / 5)\n",
    "        \n",
    "train_model_cuda(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason you want to run your code on a specific GPU the easiest way to do that is to just set an environment variable when running your code. \n",
    "\n",
    "CUDA_VISIBLE_DEVICES=2 python3 my_nn_code.py\n",
    "\n",
    "The above code would force your python file to only see GPU with ID 2 and would run your code there. This is helpful if another GPU is already full or you just want to run a lot of models with different parameters to see what gives better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what if my model is taking a long time even on the GPU, can I use a distributed setup?\n",
    "\n",
    "Yes -- you can parallelize your code across multiple GPUs but this is a more advanced topic. PyTorch offers some easy ways to at least get some paralleization out of your code but it comes down to you as a programmer to some extent. If interested I recommend checking out this [guide](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
